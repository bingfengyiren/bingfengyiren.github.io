---
layout:     post
title:      "「翻译」ERNIE 2.0"
subtitle:	" 持续语言理解预训练框架"
date:       2020-01-09 20:00:00
author:     "cuiming"
header-img: "img/post-pretrain-ernie.jpg"
catalog: true
tags:	
   - 预训练
   - NLP
---

> A Continual Pre-Training Framework for Language Understanding

#### 摘要

​		最近预训练模型在各种各样的语言理解任务上都获得了最好的结果。<font color="#DC3540">当前的预训练任务都是通过几个简单的任务来训练模型，从而捕获单词或句子的共现信息。但是除了共现信息外，训练语料中还包含了词法、句法以及语义等有价值的信息。为了从训练预料中提取词法、句法和语义信息，论文提出了一个名为ERNIE 2.0的改进预训练框架，该框架以递进的方式构建预训练任务，然后通过多任务学习的方式在这些任务上预训练模型。</font>>在此框架的基础上，论文构建多个任务并训练ERINE2.0 模型，以捕获训练数据中的词法、句法和语义方面的信息。实验结果表明ERNIE 2.0 模型在16个任务上优于BERT和XLNET，包括GLUE英文任务以及一些类似的中文任务。[项目源码和预训练模型地址](https://github.com/PaddlePaddle/ERNIE.)

#### 引言

预训练语言表示比如ELMO，OpenAI GPT，BERT，ERNIE 1.0以及XLNET已经被证明在各种各样的语言理解任务中都有很好的效果，比如情感分类，自然语言推理（Natural Language Inference，NLI）,命名实体识别等等。

通常预训练模型都是基于单词和句子的共现来训练。但是在训练预料中的除了共现信息外，词法、句法和语义信息同样值得提取。诸如人名、地名、机构名等命名实体可能包含一些概念信息。

ERNIE框架支持通过不断进行多任务学习来不断引入各种定制任务。 当执行一项或多项新任务时，连续多任务学习方法可以有效地同时训练新引入的任务和原始任务，而不会忘记先前学习的知识。 这样，我们的框架可以根据之前掌握的参数训练增量式分布的表示形式。 此外，在此框架中，所有任务共享相同的编码网络，从而使跨不同任务的词汇，句法和语义信息的编码成为可能。

论文的贡献主要体现在以下几个方面：

- 论文提出一种改进的预训练框架ERINE 2.0，该框架可以以增量的方式支持定制化训练任务和改进的多任务学习。
- 论文构建了三类的非监督语言处理任务用于验证该框架的有效性。实验结果表明ERNIE 2.0 模型在16个任务上优于BERT和XLNET。
- ERNIE 2.0基于英文的预训练和fine-tuning[代码地址](https://github.com/PaddlePaddle/ERNIE)

#### 相关工作

1. 语言表示的无监督学习

   用大量的无标注数据，通过预训练语言模型的方式学习通用的语言表示是高效的。传统方法通常专注于上下文无关的词嵌入。Word2Vec（Mikolov等人，2013）和GloVe（Pennington，Socher和Manning，2014）等方法基于大型语料库中单词共现信息来学习固定的词嵌入。

   最近，提出了一些上下文相关的语言表示方式，在各类的NLP任务中都取得了最好的结果。ELMo（Peters et al.2018）建议从语言模型中提取上下文相关的功能。OpenAI GPT（Radford等人，2018）通过调整Transformer（Vaswani等人，2017）增强了上下文相关的嵌入表示。BERT（Devlin等人，2018）采用了遮掩语言模型(Masked language model,MLM)，同时在预训练中添加了下一个句子预测任务。XLM (Lample and Conneau 2019)集成了依赖单语言数据的无监督学习方法和利用平行双语数据的监督学习方法，来学习跨语言的语言模型。MT-DNN(Liu et al. 2019)在预训练模型的基础上学习了一些GLUE中的监督学习任务取得了更好的结果，相比于其它预训练模型而言它在其它监督学习任务上也表现更为优异。<font color='blue'>XLNET (Yang et al. 2019)使用Transformer-XL((Dai et al. 2019)并提出了一种通用自回归预训练方法来学习双向上下文。</font>

2. 持续学习(Continual Learning,CL)

   持续学习旨在按顺序训练几个任务的模型，以便在学习新任务时模型仍然记得先前学习的任务。这些方法受到人类学习过程的启发，因为人类能够通过不断积累学习到的信息，从而有效的开发新技能。通过持续学习，由于先前训练中获得的知识，该模型应该能够在新任务上表现更好。

#### ERNIE 2.0 框架

![image-20200109143946566](/img/ernie/image-20200109143946566.png)

如图1所示，ERNIE2.0框架是基于广泛使用的预训练和微调架构构建的。与其它以少量的预训练目标进行训练不同的是，ERNIE2.0可以不断的引入大量各种各样的预训练任务，以帮助模型有效的学习词汇，句法和语义信息。基于此，ERNIE2.0框架通过持续多任务学习不断更新预训练模型。微调的时候，首先使用预先训练的参数初始化ERNIE模型，然后再使用来自特定任务的数据进行微调。

<center class="half"> 
	<table border="0" align="center" cellpadding="0" cellspacing="0"><tr><td>
	<img src="/img/ernie/image-20200109175453662.png" alt="image-20200109175453662.png" style="zoom:80%"/></td><td> 
	<img src="/img/ernie/image-20200109175513036.png" alt="image-20200109175513036.png" style="zoom:50%"/> </td></tr></table>
</center>

1. 持续预训练

   持续预训练的过程包含两个步骤：1）不断构建设计大数据和先验知识的无监督预训练任务 2）通过持续的多任务学习不断的更新ERNIE模型

   <b><font color='LightSeaGreen'>预训练任务构建</font></b>  我们每次可以构建不同类型的任务，包括单词感知任务、结构感知任务和语义感知任务。所有这些预训练任务都依赖于自监督或者弱监督信号，这些信号可以从海量数据中获取，而无需人工标注。诸如命名实体，短语，篇章关系之类的先验知识可用于从大规模数据中生成标签。

   <b><font color='LightSeaGreen'>持续多任务学习</font></b> ERNIE 2.0 框架旨在从不同任务中学习词法、句法与语法信息。因此，有两个主要挑战需要客服。1）如何以连续的方式训练任务而又不会忘记以前学到的知识 2）如何有效的预训练这些任务。我们提出了一种持续的多任务学习方法来解决这两个问题。每当有新任务出现时，持续多任务学习方法首先使用先前学习的参数来初始化模型，然后同时训练新引入的任务和原始任务。这将确保学习的参数对先前学习的知识进行记忆。我们通过为每个任务分配N个训练迭代来解决训练效率的问题。框架需要针对每个任务自动将这N次迭代分配给不同阶段的训练。通过这种方式，我们既可以保证模型的训练效率，又不会忘记先前训练学习到的知识。
   
   ​		图2说明了论文中的方法与多任务学习和持续学习的区别。尽管从头进行多任务学习可以同时学习多个任务，但是在训练之前必须好所有定制的预训练任务。因此这种方法所花费的时间和持续学习花费的时间一样多，甚至会更多。传统的持续学习方法在每个阶段仅通过一项任务训练模型，这样的缺点是它可能会遗忘先前学习到的知识。
   
   ​		如图4所示，我们在每个阶段持续进行的多任务学习的体系结构包含一系列共享文本编码层，以对上下文信息进行编码，这些层可以使用RNN或多层自注意力层堆叠的深度Transformer。编码层的参数可以被所有学习任务更新。论文框架中有两种损失函数一个是句子级别的损失函数，另一个是词级别的损失，这个和BERT的损失函数类似。每个预训练任务都有自己的随时函数，在预训练期间，可以将句子级别的损失函数与词级别的损失结合起来以不断更新模型。

2. 针对实际任务进行fine-tuning

   通过对特定任务的监督数据进行微调，预训练模型能够更好的适应不同的语言理解任务，比如QA(question answering)，NLI(Natural Language Inference)和语义相似度。每个下游任务都有针对各自任务fine-tuning后的模型。

#### ERNIE 2.0模型

为了验证框架的有效性，论文构建的三种不同的无监督语言处理任务并开发了ERNIE 2.0预训练模型。本节主要介绍模型的实现。



![image-20200109194729209](/img/ernie/image-20200109194729209.png)

1. 模型结构

   <b><font color='LightSeaGreen'>Tranformer Encoder </font></b> 该模型使用多层Transformer作为基本编码器，就像其它预训练模型一样，比如GPT，BERT，XLM。transformer可以通过自注意力机制捕获序列中每个词的上下文信息，并生成一系列上下文相关的嵌入向量。给定一个序列，添加特殊的标识符[CLS]到序列的第一个位置。此外，在多个输入分段任务时，用符号[SEP]作为输入段的分隔符。

   <b><font color='LightSeaGreen'>Task Embedding</font></b>  模型通过提供任务向量来表示不同任务的特点。我们用0到N的ID表示不同的任务。每一个任务id都被赋予了唯一一个任务向量。对应的词，片段，位置和任务向量被作为模型的输入。在fine-tuning阶段可以用任意的任务ID来初始化模型。模型结构如图3所示。

   <b><font color='LightSeaGreen'>Pre-training Tasks</font></b>  我们构建了三种不同的任务来获取训练语料不同方面的信息。单词感知任务来捕获词法信息，结构感知任务来不说句法信息，语义感知任务用来捕获语义信息。

2. 词感知预训练任务

   <b><font color='LightSeaGreen'>Knowledge Tasks</font></b>  ERNIE 1.0 提出了一种知识整合的策略来增强表示。它引入了短语屏蔽和命名实体屏蔽，并预测了整个被屏蔽的短语和命名实体，以帮助模型学习局部上下文和全局上下文中的依赖信息。 我们使用此任务来训练模型的初始版本。

   <b><font color='LightSeaGreen' style="font-family:SimHei">大写预测任务</font></b> 大写单词相比于句子中的其它词而言通常包含特定的语义信息。示例模型在诸如命名实体识别之类的任务中具有一些优势，而非示例模型则更适合其他一些任务。 为了结合两种模型的优点，我们添加了一项任务来预测单词是否大写。

   <b><font color='LightSeaGreen' style="font-family:SimHei">词文档关系预测任务</font></b>  该任务预测一个片段中的词是否出现在原始文档的其它片段中。从经验上讲，出现在文档许多片段中的单词通常是常用单词或与文档主题相关的相关的。因此通过识别在文档片段中频繁出现的词，该任务可以可以使模型能够在某种程度上捕获文档的关键词信息。

3. 结构感知预训练任务

   <b><font color='LightSeaGreen' style="font-family:SimHei">句子重排序任务</font></b>  该任务旨在学习句子之间的关系。在此任务的预训练过程中，将给定的段落随机分为1到m个片段，然后所有的所有的片段随机排列组合。预训练任务的目标就是把这些片段重新组织，这个问题可以建模为一个k分类问题其中 $k=\sum_{n=1}^{m} n !$，从经验来看句子重排序任务可以使预训练模型学习文档中句子之间的关系。

   <b><font color='LightSeaGreen' style="font-family:SimHei">句子距离任务</font></b>  我们还构建了一个预训练任务用文档级信息来学习句子距离。该任务被建模为3分类问题，0表示两个句子在同一个文档中并相邻，1表示两个句子在同一个文档中但不相邻，2表示两个句子来源于不同的文档。

4. 语义感知预训练任务

   <b><font color='LightSeaGreen'>Discourse Relation Task</font></b>  除了句子距离以外，还引入了一个学习两个句子之间语义和修饰关系的任务。我们使用Sileo等人构建的数据来训练针对英语任务的预训练模型。按照Sileo等人使用的方法，我们还自动构建了一个中文数据集进行预训练。

   <b><font color='LightSeaGreen'>IR Relevance Task </font></b>  我们构建了一个关于query与title关系的3分类任务来学习信息检索中的短文本相关性。我们将查询作为第一句话，标题作为第二句话，商业搜索引擎的搜索日志数据作为预训练数据。此任务中有3中标签，"0"表示query和title强相关，意味着用户在输入查询后点击标题；"1"表示不可靠，意味着用户输入查询时，这些标题会显示在搜索结果中，但不会被点击；"2"表示查询和标题在语义信息方面是完全无关和随机的。

#### 实验

论文对比分析了ERNIE 2.0与效果最好的预训练模型。对于英文任务，对比了ERNIE 2.0，BERT，XLNET在GLUE任务上的效果。对于中文任务，对比分析了ERNIE 2.0，BERT，ERNIE 1.0在多个中文数据集上的效果。此外，我们还将我们的方法与多任务学习和传统的持续学习进行比较。

![image-20200110102237582](/img/ernie/image-20200110102237582.png)

1. 预训练和实现

   <b><font color='LightSeaGreen' style="font-family:SimHei">预训练数据</font></b>  和BERT类似，英文语料中的有些数据是从Wikipedia和BookCorpus爬取过来的。此外我们还从Reddit上收集了一些数据，并用Disovery数据作为篇章关系数据。对于中文语料，我们从搜索引擎收集了各种各样的数据，例如百科全书，新闻，对话，信息检索和篇章关系数据。关于训练数据的详细信息如表2所示，预训练数据和训练任务的关系如表1所示。

   ![image-20200110101211692](/img/ernie/image-20200110101211692.png)

   <b><font color='LightSeaGreen' style="font-family:SimHei">预训模型参数</font></b>  为了和BERT模型对比，我们使用和BERT相同的参数设置。base版模型包含12层，12个自注意力头和隐藏层的维度是768维；large版模型包含24层，16个自注意力头，隐藏层的的维度是1024维。XLNET模型的设置和BERT相同。

<center class="half"> 
	<table border="0" align="center" cellpadding="0" cellspacing="0"><tr><td>
	<img src="/img/ernie/image-20200110102809661.png" alt="image-20200109175453662.png" style="zoom:80%"/></td><td> 
	<img src="/img/ernie/image-20200110103143386.png" alt="image-20200109175513036.png" style="zoom:80%"/> </td></tr></table>
</center>

   ERNIE 2.0 base版模型在48块NVidia v100 GPU卡上训练，large版模型用了64块NVidia v100 GPU。模型在百度开发的端到端预训练平台PaddlePaddle上训练。优化器选择Adam, 其中$\beta_{1}=0.9,\beta_{2}=0.98$ ,英文模型的学习率为5e-5，中文模型学习率为1.28e-4，每个预训练任务的前4000步预热。通过float16运算，我们设法加快了训练速度并减少了模型的内存使用量。 对每个预训练任务进行训练，直到预训练任务的度量收敛为止。

2. 任务微调

   <b><font color='LightSeaGreen' style="font-family:SimHei">英文任务</font></b> 作为自然语言理解的多任务基准和分析平台，GLUE通常用于评估模型的性能。 我们还在GLUE上测试了ERNIE 2.0的性能。 具体而言，GLUE涵盖了各种NLP数据集，其详细信息如下：

   ![image-20200110182418741](/img/ernie/image-20200110182418741.png)

   <b><font color='LightSeaGreen' style="font-family:SimHei">中文任务</font></b> 我们对9个中文NLP任务进行了广泛的实验，包括机器阅读理解，命名实体识别，自然语言推论，语义相似度，情感分析和QA。 具体来说，选择以下中文数据集来评估ERNIE 2.0在中文任务上的效果：

   - 机器阅读理解(MRC):CMRC(2018)
   - 命名实体识别(NER):MSRA-NER(2006)
   - 情感分析(SA):ChnSentiCorp
   - 语义相似度(SS):LCQMC
   - 问答(QA):NLPCC-DBQA

     ![](/img/ernie/image-20200110183040339.png)

#### 结论

论文提出了一种名为ERNINE 2.0的持续预训练框架，在该框架下可以通过持续学习的方式逐步构建与学习预训练任务。在此框架下训练了涵盖语言不同方面的预训练模型。
